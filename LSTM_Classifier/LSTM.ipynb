{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start building LSTM for classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "Load data from S3 buckets\n",
    "role = get_execution_role()\n",
    "bucket='thesisdatabucketad'\n",
    "data_key = 'truth_data.csv'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "\n",
    "bucket2='thesisdatabucketad'\n",
    "data_key2 = 'model_data.csv'\n",
    "data_location2 = 's3://{}/{}'.format(bucket2, data_key2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_location)\n",
    "df.head()\n",
    "\n",
    "#remove stpo words from text\n",
    "stop = set(stopwords.words('english'))\n",
    "df['text2'] = df['text'].str.lower().str.split()\n",
    "df['text2'] = df['text2'].apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "\n",
    "df = df.rename(columns = {'Sent Rating Sean':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reset classifications to non-numeric (easier process)\n",
    "df['new_sent'] = df.sentiment.map({-2:'negative',-1:'negative',0:'neutral',1:'positive',2:'positive'})\n",
    "df = df[['ticks', 'date','text2', 'new_sent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lists \n",
    "input_sentences = [text for text in df[\"text2\"].values.tolist()]\n",
    "labels = df[\"new_sent\"].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = dict()\n",
    "label2id = dict()\n",
    "\n",
    "max_words = 0 # maximum number of words in a sentence\n",
    "\n",
    "# Construction of word2id dict\n",
    "for sentence in input_sentences:\n",
    "    for word in sentence:\n",
    "        # Add words to word2id dict if not exist\n",
    "        if word not in word2id:\n",
    "            word2id[word] = len(word2id)\n",
    "    # If length of the sentence is greater than max_words, update max_words\n",
    "    if len(sentence) > max_words:\n",
    "        max_words = len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'positive', 1: 'neutral', 2: 'negative'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#turn classifications to numeric values for LSTM to process\n",
    "label2id = {l: i for i, l in enumerate(set(labels))}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (2500, 89)\n",
      "Shape of Y: (2500, 3)\n"
     ]
    }
   ],
   "source": [
    "# Encode input words and labels\n",
    "X = [[word2id[word] for word in sentence] for sentence in input_sentences]\n",
    "Y = [label2id[label] for label in labels]\n",
    "\n",
    "\n",
    "# Apply Padding to X\n",
    "\n",
    "X = pad_sequences(X, max_words)\n",
    "\n",
    "# Convert Y to numpy array\n",
    "Y = keras.utils.to_categorical(Y, num_classes=len(label2id))\n",
    "\n",
    "# Print shapes\n",
    "print(\"Shape of X: {}\".format(X.shape))\n",
    "print(\"Shape of Y: {}\".format(Y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balance training data due to large amount of bias\n",
    "Z = Y[:,0]\n",
    "weight = class_weight.compute_class_weight('balanced', np.unique(Z), Z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 89)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 89, 100)      867700      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 89, 100)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 89, 200)      160800      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 89, 200)      0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 89, 1)        201         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 89)           0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Activation)      (None, 89)           0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 200)          0           dropout_2[0][0]                  \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          20100       dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 3)            303         dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,049,104\n",
      "Trainable params: 1,049,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100 \n",
    "\n",
    "# Input Tensor\n",
    "sequence_input = keras.Input(shape=(max_words,), dtype='int32')\n",
    "\n",
    "# Word embedding\n",
    "embedded_inputs =keras.layers.Embedding(len(word2id) + 1,\n",
    "                                        embedding_dim,\n",
    "                                        input_length=max_words)(sequence_input)\n",
    "\n",
    "# Apply dropout to prevent overfitting\n",
    "embedded_inputs = keras.layers.Dropout(0.1)(embedded_inputs)\n",
    "\n",
    "# Apply Bidirectional LSTM over embedded inputs\n",
    "lstm_outs = keras.layers.wrappers.Bidirectional(\n",
    "    keras.layers.LSTM(embedding_dim, return_sequences=True)\n",
    ")(embedded_inputs)\n",
    "\n",
    "# Apply dropout to LSTM outputs to prevent overfitting\n",
    "lstm_outs = keras.layers.Dropout(0.1)(lstm_outs)\n",
    "\n",
    "# Attention Mechanism - Generate attention vectors\n",
    "input_dim = int(lstm_outs.shape[2])\n",
    "permuted_inputs = keras.layers.Permute((2, 1))(lstm_outs)\n",
    "attention_vector = keras.layers.TimeDistributed(keras.layers.Dense(1))(lstm_outs)\n",
    "attention_vector = keras.layers.Reshape((max_words,))(attention_vector)\n",
    "attention_vector = keras.layers.Activation('softmax', name='attention_vec')(attention_vector)\n",
    "attention_output = keras.layers.Dot(axes=1)([lstm_outs, attention_vector])\n",
    "\n",
    "# Last layer: softmax activation\n",
    "fc = keras.layers.Dense(embedding_dim, activation='relu')(attention_output)\n",
    "output = keras.layers.Dense(len(label2id), activation='softmax')(fc)\n",
    "\n",
    "# Building model\n",
    "model = keras.Model(inputs=[sequence_input], outputs=output)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2025 samples, validate on 225 samples\n",
      "Epoch 1/3\n",
      "2025/2025 [==============================] - 23s 11ms/step - loss: 0.8085 - acc: 0.7027 - val_loss: 0.8021 - val_acc: 0.6711\n",
      "Epoch 2/3\n",
      "2025/2025 [==============================] - 18s 9ms/step - loss: 0.6470 - acc: 0.7249 - val_loss: 0.6663 - val_acc: 0.7244\n",
      "Epoch 3/3\n",
      "2025/2025 [==============================] - 17s 8ms/step - loss: 0.3761 - acc: 0.8622 - val_loss: 0.8670 - val_acc: 0.7067\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f065854bf90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data and fit model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=123)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.1, shuffle=True, class_weight= weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing outputs of y_test x_test (visual check of data) \n",
    "test = model.predict(X_test)\n",
    "dftest = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21  27   0]\n",
      " [ 23 154   0]\n",
      " [  7  18   0]]\n"
     ]
    }
   ],
   "source": [
    "#Confusion Matrix Validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = np.argmax(test, axis =1)\n",
    "y_test2 = np.argmax(y_test, axis = 1)\n",
    "\n",
    "cm = confusion_matrix(y_test2, pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results yeild weak recall for Negative sentiment classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>Good day, everyone, and welcome to the Amazon....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>At this time, all participants are in a listen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>After the presentation, we will conduct a ques...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>Today's call is being recorded.For opening rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>Please, go ahead.Darin Manney   Amazon.com, In...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date                                          tokenized\n",
       "0   AMZN  02/02/2017  Good day, everyone, and welcome to the Amazon....\n",
       "1   AMZN  02/02/2017  At this time, all participants are in a listen...\n",
       "2   AMZN  02/02/2017  After the presentation, we will conduct a ques...\n",
       "3   AMZN  02/02/2017  Today's call is being recorded.For opening rem...\n",
       "4   AMZN  02/02/2017  Please, go ahead.Darin Manney   Amazon.com, In..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#earnings calls dataset\n",
    "df_calls = pd.read_csv(data_location2)\n",
    "df_calls = df_calls[['ticker','date','tokenized']]\n",
    "df_calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>[good, day,, everyone,, welcome, amazon.com, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>[time,, participants, listen, mode.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>[presentation,, conduct, question, answer, ses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>[today's, call, recorded.for, opening, remarks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>02/02/2017</td>\n",
       "      <td>[please,, go, ahead.darin, manney, amazon.com,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker        date                                          tokenized\n",
       "0   AMZN  02/02/2017  [good, day,, everyone,, welcome, amazon.com, q...\n",
       "1   AMZN  02/02/2017               [time,, participants, listen, mode.]\n",
       "2   AMZN  02/02/2017  [presentation,, conduct, question, answer, ses...\n",
       "3   AMZN  02/02/2017  [today's, call, recorded.for, opening, remarks...\n",
       "4   AMZN  02/02/2017  [please,, go, ahead.darin, manney, amazon.com,..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words from text\n",
    "df_calls['tokenized'] = df_calls['tokenized'].str.lower().str.split()\n",
    "df_calls['tokenized'] = df_calls['tokenized'].apply(lambda x: [item for item in x if item not in stop])\n",
    "df_calls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for keras model. \n",
    "input_sentences2 = [text for text in df_calls[\"tokenized\"].values.tolist()]\n",
    "\n",
    "word2id2 = dict()\n",
    "\n",
    "max_words2 = 0 # maximum number of words in a sentence\n",
    "\n",
    "# Construction of word2id dict\n",
    "for sent in input_sentences2:\n",
    "    for word in sent:\n",
    "        if word not in word2id2:\n",
    "            word2id2[word] = len(word2id2)\n",
    "    if len(sent) > max_words2:\n",
    "        max_words2 = len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X2: (1919467, 89)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X2 = [[word2id2[word] for word in sent] for sentence in input_sentences2]\n",
    "\n",
    "X2 = pad_sequences(X2, max_words)\n",
    "print(\"Shape of X2: {}\".format(X2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9979931e-01 3.3908800e-12 2.3848116e-13 ... 8.8095904e-13\n",
      "  4.8229337e-12 2.0069852e-04]\n",
      " [9.9979931e-01 3.3908800e-12 2.3848116e-13 ... 8.8095904e-13\n",
      "  4.8229337e-12 2.0069852e-04]\n",
      " [9.9979931e-01 3.3908800e-12 2.3848116e-13 ... 8.8095904e-13\n",
      "  4.8229337e-12 2.0069852e-04]\n",
      " ...\n",
      " [9.9979931e-01 3.3908800e-12 2.3848116e-13 ... 8.8095904e-13\n",
      "  4.8229337e-12 2.0069852e-04]\n",
      " [9.9979931e-01 3.3908800e-12 2.3848116e-13 ... 8.8095904e-13\n",
      "  4.8229337e-12 2.0069852e-04]\n",
      " [9.9979931e-01 3.3908800e-12 2.3848116e-13 ... 8.8095904e-13\n",
      "  4.8229337e-12 2.0069852e-04]]\n"
     ]
    }
   ],
   "source": [
    "#predict data (Tested LSTM against full dataset anyway)\n",
    "prediction = model.predict(X2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
