{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextProcessor",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ljfiScLxfFZP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk \n",
        "import matplotlib as plot\n",
        "import seaborn as sns\n",
        "# nltk.download()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zO_zuhVHyOgz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/drive/My Drive/root/test_output\", sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oySR9JG-i0Q2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Filter data and clean data. Followed by NLTK sentence tokenizer. Once data has been tokenized the documents have been left as w list with in a column. I utilized what is known as dataframe explosion to break the list into seperate rows while maintaining the axis (ticker & date)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Phdwgs4fCn51",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Filter for desired tickers and timeframe\n",
        "df = df[df['publish_on'] > '2017-01-01 00:00:00']\n",
        "df = df[df['primary_ticker'].str.contains('NasdaqGS')]\n",
        "\n",
        "#Clean data for tokenizing \n",
        "df['content'] = df['content'].str.replace('<[^<]+?>', '')\n",
        "df['content'] = [re.sub(\"[^A-Za-z0-9,.']\", \" \", x) for x in df['content']]\n",
        "\n",
        "#Tokenize data by sentence\n",
        "df['tokenized'] = df.apply(lambda row: nltk.sent_tokenize(row['content']), axis =1)\n",
        "df = df[[ 'primary_ticker', 'publish_on','tokenized']] \n",
        "\n",
        "#DataFrame Explode\n",
        "rows= []\n",
        "_ = df.apply(lambda row: [rows.append([row['primary_ticker'], row['publish_on'], nn]) for nn in row.tokenized], axis=1)\n",
        "df_new = pd.DataFrame(rows, columns=df.columns).set_index(['primary_ticker', 'publish_on'])\n",
        "\n",
        "df_new = df_new.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMcwjbDDkid7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize distribution of sentences length by character. \n"
      ]
    },
    {
      "metadata": {
        "id": "iFcYapD1C7Lb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "df_new['text_count'] = df_new['tokenized'].str.len() \n",
        "print(\"mean \" + str(df_new['text_count'].mean()))\n",
        "print(\"median \" + str(df_new['text_count'].median()))\n",
        "\n",
        "sns.distplot(df_new['text_count'])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zre-fWqsko3c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Trying to determine a sentence length that can be removed from the dataset sentences like \"thank you next question\" are unimportant for the anlaysis"
      ]
    },
    {
      "metadata": {
        "id": "9UJd9422CtAG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q1 = df_new['text_count'].quantile(0.1)\n",
        "q3 = df_new['text_count'].quantile(.95)\n",
        "iqr = q3-q1\n",
        "\n",
        "print(q1)\n",
        "print(q3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b3oiQQH8k41m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Femove any undesired tickers due. Also apply a minimum length for sentences"
      ]
    },
    {
      "metadata": {
        "id": "l7xiWqzCDlbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Apply minimum length of characters \n",
        "df2 = df_new[df_new['tokenized'].apply(lambda x: len(x) > q1)]\n",
        "# df2 = df2[df2['tokenized'].apply(lambda x: len(x) <= q3)]\n",
        "\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:AMSW.A']\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:ASCM.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:BELF.B' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:CMCS.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:DISC.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:GNCM.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:IMKT.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:KELY.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:LBTY.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:NRCI.B' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:STRZ.A' ]\n",
        "df2 = df2[df2['primary_ticker'] != 'NasdaqGS:VLCC.F' ]\n",
        "\n",
        "df2['publish_on'] = pd.to_datetime(df2['publish_on'])\n",
        "df2['date']= df2['publish_on'].apply(lambda x:x.date().strftime('%m/%d/%Y'))\n",
        "df2['exchange'], df2['ticker'] = df2['primary_ticker'].str.split(':', 1).str\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XIJG20YulF6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unload CSV in order to save processing time from original text file that is 6GB\n"
      ]
    },
    {
      "metadata": {
        "id": "O9lAasAOFGZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "df2.to_csv('/content/drive/My Drive/root/text_model.csv')\n",
        "# files.download('.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nUe0YhdDRRX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qCAHywlBE2Xj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}